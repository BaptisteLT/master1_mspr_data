import os
import pandas as pd
import random
import numpy as np
import unicodedata
from sqlalchemy import create_engine, Table, Column, Integer, String, Float, ForeignKey, MetaData, text, select
from sklearn.preprocessing import MinMaxScaler, StandardScaler


# Database connection
DB_URL = "mysql+pymysql://root:@localhost:3306/elections"
engine = create_engine(DB_URL)
metadata = MetaData()
metadata.reflect(bind=engine)
conn = engine.connect()


# Load tables
departement = metadata.tables["departement"]
elections = metadata.tables["elections"]

#TODO: √† mettre dans le doc: 
# D√©partements et vos associ√©s au d√©partement √† ne pas ins√©rer en BDD car il manque des donn√©es du ch√¥mage par exemple (pour MAYOTTE)
departements_to_ignore = {"MAYOTTE", "FRAN√áAIS √âTABLIS HORS DE FRANCE", "NOUVELLE CALEDONIE", "SAINT-MARTIN/SAINT-BARTH√âLEMY", "SAINT-PIERRE-ET-MIQUELON", "POLYNESIE FRANCAISE","POLYN√âSIE FRAN√áAISE", "NOUVELLE-CAL√âDONIE",  "SAINT PIERRE ET MIQUELON", "SAINT-MARTIN/SAINT-BARTHELEMY", "WALLIS ET FUTUNA", "WALLIS-ET-FUTUNA", "FRANCAIS DE L'ETRANGER"}


# D√©finir la table 'type_de_position'
type_de_position = Table(
    "type_de_position", metadata,
    autoload_with=engine  # Charge automatiquement la structure de la table depuis la BDD
)

# Fonction pour r√©cup√©rer l'ID de position bas√© sur le libell√© depuis la base de donn√©es
def get_position_id(libelle):
    
    try:
        # Ex√©cution de la requ√™te pour r√©cup√©rer l'ID bas√© sur le libell√©
        result = conn.execute(
            select(type_de_position.c.id)
            .where(type_de_position.c.libelle == libelle)
        ).fetchone()  # R√©cup√®re le premier r√©sultat
    
        if result:
            print(result)
            return result[0]  # Retourne l'ID trouv√©
        else:
            raise ValueError(f"Libelle '{result}' invalide")
    except Exception as e:
        print(f"Erreur lors de la r√©cup√©ration de l'ID: {e}")
        return None

# Fonction pour r√©cup√©rer l'ID de la position bas√©e sur le nom et pr√©nom
def get_position_type(nom_gagnant, prenom_gagnant):
   
    position_map = {
        "emmanuel macron": "milieu",
        "fran√ßois hollande": "gauche",
        "nicolas sarkozy": "droite",
        "jacques chirac" : "droite"
    }

    full_name = f"{prenom_gagnant} {nom_gagnant}".lower()
    # V√©rifie si le candidat existe dans le dictionnaire
    position = position_map.get(full_name)
    if position is not None:
        # R√©cup√®re l'ID correspondant √† la position dans la base de donn√©es
        position_id = get_position_id(position)
        print(f"position_id {position_id}")
        
        if position_id:
            return int(position_id)  # Retourne l'ID de la position
        else:
            # Raise error si l'id n'a pas √©t√© trouv√© en BDD
            raise ValueError(f"{position_id} not found in database")  
    else:
        raise ValueError(f"Candidat '{full_name}' non trouv√© dans le dictionnaire")
    



# Effacer toutes les tables au d√©but du script
def truncate_tables(database_url, tables):
    """
    Truncate the given list of tables in the specified database.

    :param database_url: str - The SQLAlchemy database connection URL.
    :param tables: list - A list of table names to truncate.
    """
    engine = create_engine(database_url)
    
    
    for table in tables:
        conn.execute(text(f"TRUNCATE TABLE {table}"))
    conn.commit()

    print(f"Truncated tables: {', '.join(tables)}")

def convert_to_csv(file_path):
    """Convertion des fichiers Excel en CSV, et les sauvegarder dans ./data/votes/csvs."""
    csv_folder = os.path.join(os.path.dirname(file_path), "csvs")
    os.makedirs(csv_folder, exist_ok=True)  # Si le dossier n'existe pas, on le cr√©e

    file_name = os.path.basename(file_path).replace(".xlsx", ".csv").replace(".xls", ".csv")
    csv_path = os.path.join(csv_folder, file_name)

    try:
        if file_path.endswith(".xlsx"):
            xls = pd.ExcelFile(file_path, engine="openpyxl")
        elif file_path.endswith(".xls"):
            xls = pd.ExcelFile(file_path, engine="xlrd")
        else:
            print(f"‚ö†Ô∏è N'est pas dans un format Excel: {file_path}")
            return None

        # Afficher les fiches disponibles (pour d√©bugger)
        print(f"üìÑ Available sheets in {file_name}: {xls.sheet_names}")

        # Nom de la fiche √† r√©cup√©rer
        sheet_name = None
        if "R√©sultats par niveau Dpt T2 Fra" in xls.sheet_names:
            sheet_name = "R√©sultats par niveau Dpt T2 Fra"
        elif "D√©partements Tour 2" in xls.sheet_names:
            sheet_name = "D√©partements Tour 2"
        elif "D√©partements T2" in xls.sheet_names:
            sheet_name = "D√©partements T2"

        if not sheet_name:
            print(f"‚ùå La fiche n'a pas √©t√© trouv√©e dans {file_name}.")
            return None

        # Lecture de la bonne fiche
        df = pd.read_excel(xls, sheet_name=sheet_name, 
                           engine="openpyxl" if file_path.endswith(".xlsx") else "xlrd")
        
        df.to_csv(csv_path, index=False, sep=';')

        print(f"‚úÖ Fichier  {file_name} converti en CSV: {csv_path}")
        return csv_path

    except Exception as e:
        print(f"‚ùå Error processing {file_path}: {e}")
 
    return None



def get_average_age(year):
    # Chargement des donn√©es de l'√¢ge
    age_file = "./data/donnees_croisees/age_population.csv"
    age_df = pd.read_csv(age_file, sep=",", dtype=str)  # Load CSV as strings
    age_df["Ann√©e"] = age_df["Ann√©e"].astype(int)  # Convertir "Ann√©e" en integer
    age_df["√Çge moyen Ensemble"] = age_df["√Çge moyen Ensemble"].str.replace(",", ".").astype(float)

    """R√©cup√©ration de l'√¢ge moyen pour une ann√©e donn√©e."""
    row = age_df[age_df["Ann√©e"] == year]
    if row.empty:
        raise ValueError(f"No age data found for year {year}. Stopping execution.")

    return row["√Çge moyen Ensemble"].values[0]

def get_average_temperature(year):
    # Charger les donn√©es de temp√©rature
    age_file = "./data/donnees_croisees/rechauffement_planete.csv"
    # Lire le fichier CSV en traitant toutes les colonnes comme des cha√Ænes de caract√®res
    age_df = pd.read_csv(age_file, sep=",", dtype=str)
    age_df["Year"] = age_df["Year"].astype(int) # Convertir la colonne "Year" en entier
    # Nettoyer et convertir la colonne "J-D" (qui contient les temp√©ratures moyennes annuelles)
    age_df["J-D"] = (
        age_df["J-D"]
        .astype(str)  # S'assurer que les donn√©es sont bien des cha√Ænes
        .str.replace(",", ".")  # Remplacer les virgules par des points (standard fran√ßais ‚Üí anglais)
        # Supprimer tous les caract√®res qui ne sont pas des chiffres (\d), un point (.), ou un tiret (-)
        # Cela permet de nettoyer des symboles parasites ou autres lettres
        .replace(r"[^\d\.\-]", "", regex=True) 
        # Remplacer les cha√Ænes compos√©es uniquement de points (ex : ".", "..") par "0"
        # Cela √©vite les erreurs lors de la conversion en float
        .replace(r"^\.+$", "0", regex=True)
        # Remplacer les cha√Ænes vides par "0"
        .replace("", "0") 
        # Convertir la colonne nettoy√©e en float
        .astype(float)  # Convert to float
    )

    """R√©cup√©ration de la temp√©rature moyenne pour une ann√©e donn√©e."""
    row = age_df[age_df["Year"] == year]
    if row.empty:
        raise ValueError(f"No avg temperature data found for year {year}. Stopping execution.")

    return row["J-D"].values[0]



def get_moyenne_pouvoir_achat(year):
     # Charger les donn√©es de pouvoir d'achat
    age_file = "./data/donnees_croisees/pouvoir_achat.csv" 
    age_df = pd.read_csv(age_file, sep=",", dtype=str) # Lire le CSV en tant que cha√Ænes
    age_df["Ann√©e"] = age_df["Ann√©e"].astype(int) # Convertir l'ann√©e en entier
    age_df["Pouvoir d'achat arbitrable2 (par rapport √† l'ann√©e pr√©c√©dente en %)"] = (
        age_df["Pouvoir d'achat arbitrable2 (par rapport √† l'ann√©e pr√©c√©dente en %)"]
        .str.replace(",", ".")
        .astype(float)
    )  # Convertir en flottant
    
    """R√©cup√©rer la valeur pour l'ann√©e demand√©e."""
    row = age_df[age_df["Ann√©e"] == year]
    if row.empty:
        raise ValueError(f"No age data found for year {year} for moyenne_pouvoir_achat. Stopping execution.")

    return row["Pouvoir d'achat arbitrable2 (par rapport √† l'ann√©e pr√©c√©dente en %)"].values[0]





def remove_accents(text):
    return ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')


def get_unemployment_rate(departement, year):
    # Charger les donn√©es
    chomage_file = "./data/donnees_croisees/chomage.csv"  # Ajuste le chemin si n√©cessaire
    chomage_df = pd.read_csv(chomage_file, sep=",", dtype=str)  # Charger le CSV en cha√Ænes

    # Convertir en float uniquement si ce sont des cha√Ænes
    def safe_replace(value):
        if isinstance(value, str):
            return float(value.replace(",", "."))
        return value

    chomage_df.iloc[:, 2:] = chomage_df.iloc[:, 2:].map(safe_replace)
    # Transformer les donn√©es pour faciliter l'acc√®s
    chomage_df = chomage_df.melt(id_vars=["Code", "Libell√©"], var_name="P√©riode", value_name="Ch√¥mage")
    # Remplacer tous les "-" par des espaces dans la colonne "Libell√©"
    chomage_df["Libell√©"] = chomage_df["Libell√©"].str.replace("-", " ")
    chomage_df["Trimestre"] = chomage_df["P√©riode"].apply(lambda x: x.split("_")[0])
    chomage_df["Ann√©e"] = chomage_df["P√©riode"].apply(lambda x: int(x.split("_")[1]))
    chomage_df.drop(columns=["P√©riode"], inplace=True)

    # Retourne la moyenne annuelle du taux de ch√¥mage pour un d√©partement donn√©.
    # Si aucune donn√©e n'est trouv√©e pour l'ann√©e demand√©e, retourne la premi√®re ann√©e avec une valeur non nulle.
    departement = remove_accents(departement.replace("-", " "))

  
    # En base de donn√©es, nous avons "CORSE DU SUD", et dans le chomage.csv "CORSE SUD"
    # Il faut donc faire la correction manuellement pour retrouver la correspondance

    corrections = {
        "CORSE SUD": "CORSE DU SUD", 
    }

    departement = corrections.get(departement, departement)

    # Filtrer pour l'ann√©e demand√©e
    rows = chomage_df[(chomage_df["Libell√©"] == departement) & (chomage_df["Ann√©e"] == year)]
    
    if not rows["Ch√¥mage"].dropna().empty: return rows["Ch√¥mage"].mean()

    #On prend la premi√®re valeur de ch√¥mage trouv√©e pour la ligne correspondante si vide.
    # Si aucune donn√©e pour l'ann√©e demand√©e, chercher la premi√®re ann√©e avec une valeur
    first_valid_year = chomage_df[(chomage_df["Libell√©"] == departement) & 
                                  chomage_df["Ch√¥mage"].notna()].groupby("Ann√©e").first().reset_index()

    if not first_valid_year.empty:
        return first_valid_year.iloc[0]["Ch√¥mage"]
    raise ValueError(f"Aucune donn√©e trouv√©e pour {departement}.")


def standardize_data(DATA_TO_STANDARDIZE, numerical_columns):
    """
    Standardize the specified numerical columns of a dataset to have zero mean and unit variance using StandardScaler (Z-score).

    Args:
    - DATA_TO_STANDARDIZE (list of dicts): The dataset to be standardized, each row as a dictionary.
    - numerical_columns (list of str): The list of column names to be standardized.

    Returns:
    - The dataset with standardized values.
    """

    # Convert the rows into a 2D array with the numerical columns only
    data_to_standardize = np.array([
        [row[col] for col in numerical_columns] for row in DATA_TO_STANDARDIZE
    ])

    # Initialize the StandardScaler (for Z-score standardization)
    scaler = StandardScaler()

    # Fit and transform the data to standardize it (mean=0, std=1)
    standardized_data = scaler.fit_transform(data_to_standardize)

    # Update the original DATA_TO_STANDARDIZE with the standardized values
    for i, row in enumerate(DATA_TO_STANDARDIZE):
        for j, col in enumerate(numerical_columns):
            # Replace the original value with the standardized one (Z-score)
            row[col] = standardized_data[i, j]

    # Optionally, print the standardized data for debugging
    #for row in DATA_TO_STANDARDIZE:
        #print(row)

    return DATA_TO_STANDARDIZE


def insert_rows_to_db(DATA_TO_STANDARDIZE):
    for row in DATA_TO_STANDARDIZE:
        # Extraction des donn√©es n√©cessaires pour l'insertion en BDD
        annee = row['annee']
        departement_id = row['departement_id']
        moyenne_age = row['moyenne_age']
        moyenne_pouvoir_achat = row['moyenne_pouvoir_achat']
        taux_chomage = row['taux_chomage']
        type_de_position = row['type_de_position']
        temperature_moyenne = row['temperature_moyenne']
        nom_gagnant = row['nom_gagnant']
        prenom_gagnant = row['prenom_gagnant']
        nom_perdant = row['nom_perdant']
        prenom_perdant = row['prenom_perdant']
        pourcentage_vote_gagnant = row['pourcentage_vote_gagnant']
        pourcentage_vote_blanc = row['pourcentage_vote_blanc']
        pourcentage_abstention = row['pourcentage_abstention']

        # Insertion en BDD
        conn.execute(elections.insert().values(
            annee=annee,
            departement_id=departement_id,
            moyenne_age=moyenne_age,
            moyenne_pouvoir_achat=moyenne_pouvoir_achat,
            taux_chomage=taux_chomage,
            type_de_position=type_de_position,
            temperature_moyenne=temperature_moyenne,
            nom_gagnant=nom_gagnant,
            prenom_gagnant=prenom_gagnant,
            nom_perdant=nom_perdant,
            prenom_perdant=prenom_perdant,
            pourcentage_vote_gagnant=pourcentage_vote_gagnant,
            pourcentage_vote_blanc=pourcentage_vote_blanc,
            pourcentage_abstention=pourcentage_abstention
        ))

def process_vote_files():

    DATA_TO_STANDARDIZE = [];

    TABLES_TO_TRUNCATE = ["departement", "elections"]

    truncate_tables(DB_URL, TABLES_TO_TRUNCATE)


    """Process all vote files in ./data/votes."""
    folder = "./data/votes"
    csv_folder = os.path.join(folder, "csvs")
    os.makedirs(csv_folder, exist_ok=True)  # Ensure folder exists

    for file in os.listdir(folder):
        file_path = os.path.join(folder, file)

        # Convert to CSV if necessary
        if file.endswith(".xlsx") or file.endswith(".xls"):
            file_path = convert_to_csv(file_path)
            print(file_path)
        
        if file_path and file_path.endswith(".csv"):
            annee = int(file.split("_")[-1].split(".")[0])
            # Read the file without assuming the header row
            df_raw = pd.read_csv(file_path, sep=';', header=None, dtype=str)

            # On trouve le header en utilisant la premi√®re colonne "Code du d√©partement", car certains fichiers ont le header en ligne 1 ou 4
            header_row_index = df_raw[df_raw.eq("Code du d√©partement").any(axis=1)].index[0]

            # Cr√©ation d'un dataframe reprenant les bonnes lignes de colonnes
            df = pd.read_csv(file_path, sep=';', header=header_row_index, dtype=str)
        
        

            for _, row in df.iterrows():  # Parcours des lignes du fichier CSV
                departement_nom = row.get("Libell√© du d√©partement", "Unknown")
                departement_code = row.get("Code du d√©partement", "Unknown")

                # V√©rifie que les valeurs sont bien des cha√Ænes de caract√®res
                if not isinstance(departement_nom, str) or not isinstance(departement_code, str):
                    continue  # Passe √† la ligne suivante si une des valeurs n'est pas une cha√Æne

                departement_nom = departement_nom.strip().upper()
                departement_code = departement_code.strip().upper()
                                
                # On ignore certains d√©partements qui ont des colonnes vides et des donn√©es insuffisantes
                if departement_nom in departements_to_ignore:
                    # Process the department
                    print(f"Manually ignoring {departement_nom}")
                    continue  # Skip this iteration

                # On ignore les lignes o√π departement_nom ou departement_code est vide dans le fichier CSV
                if pd.isna(departement_nom) or pd.isna(departement_code):
                    print(f"Ignoring row with missing department data: {row}")
                    continue

                # V√©rifier si un d√©partement avec ce code OU ce nom existe d√©j√†
                result = conn.execute(
                    departement.select().where(
                        (departement.c.code == departement_code) | (departement.c.nom == departement_nom)
                    )
                ).fetchone()

                # Si le d√©partement n'existe pas, on le cr√©e en BDD
                if result is None:
                    conn.execute(departement.insert().values(code=departement_code, nom=departement_nom))
                    result = conn.execute(
                        departement.select().where(departement.c.code == departement_code)
                    ).fetchone()
















                departement_id = result[0]  # R√©cup√©rer l'ID du d√©partement en BDD

                # Extraction du nom et pr√©nom du gagnant des √©lections
                nom_gagnant = row["Nom"]
                prenom_gagnant = row["Pr√©nom"]

                # On r√©cup√®re la premi√®re colonne "% Voix/Ins" trouv√©e car parfois il peut y en avoir deux qui ont le m√™me nom "% Voix/Ins"
                first_col_index = row.index.get_loc("% Voix/Ins")  # Get first occurrence index
                value = row.iloc[first_col_index]
                # R√©cup√©ration du pourcentage vote gagnant
                pourcentage_vote_gagnant = float(value.replace(',', '.')) if isinstance(value, str) else float(value)

                possible_columns = ["% BlNuls/Ins", "% Blancs/Ins"]
                pourcentage_vote_blanc = None

                # On va trouver la premi√®re colonne disponible de possible_columns. La colonne trouv√©e sera le pourcentage vote blanc
                for col in possible_columns:
                    if col in df.columns and pd.notna(row.get(col)):  # On verifie que la colonne existe et que sa valeur n'est pas NaN
                        pourcentage_vote_blanc = row[col]
                        break

                # Si aucune colonne n'a √©t√© trouv√©e, on raise une erreur pour ajouter la bonne colonne par la suite dans possible_columns
                if pourcentage_vote_blanc is None:
                    raise ValueError("No valid column found for vote blanc percentage")

                # R√©cup√©ration du pourcentage d'abstention
                pourcentage_abstention = row["% Abs/Ins"];

                # On r√©cup√®re les 5 dernieres colonnes qui correspondent au perdant des √©lections
                loser_data = row.iloc[-5:].dropna().values  # Extract last 5 columns & drop NaN values
                # R√©cup√©ration du nom du perdant
                nom_perdant = loser_data[0]
                # R√©cup√©ration du pr√©nom du perdant
                prenom_perdant = loser_data[1]

                print(f"unemployment: {get_unemployment_rate(departement_nom, annee)} | annee: {annee} | departement: {departement_nom}")



                # Insert into elections
                '''
                conn.execute(elections.insert().values(
                    annee=annee,
                    departement_id=departement_id,
                    moyenne_age=get_average_age(annee),
                    moyenne_pouvoir_achat=get_moyenne_pouvoir_achat(annee),
                    taux_chomage=get_unemployment_rate(departement_nom, annee),
                    type_de_position=random.randint(1, 5),
                    temperature_moyenne=get_average_temperature(annee),
                    nom_gagnant=nom_gagnant,
                    prenom_gagnant=prenom_gagnant,
                    nom_perdant=nom_perdant,
                    prenom_perdant=prenom_perdant,
                    pourcentage_vote_gagnant=pourcentage_vote_gagnant,
                    #pourcentage_vote_perdant=pourcentage_vote_perdant,
                    pourcentage_vote_blanc=pourcentage_vote_blanc, 
                    pourcentage_abstention=pourcentage_abstention
                ))
                '''

                #ON remplace les , par des .
                pourcentage_vote_blanc = pourcentage_vote_blanc.replace(',', '.')
                pourcentage_abstention = pourcentage_abstention.replace(',', '.')
        

                DATA_TO_STANDARDIZE.append({
                    "annee": annee,
                    "departement_id": departement_id,
                    "moyenne_age": np.float64(get_average_age(annee)),
                    "moyenne_pouvoir_achat": np.float64(get_moyenne_pouvoir_achat(annee)),
                    "taux_chomage": np.float64(get_unemployment_rate(departement_nom, annee)),
                    "temperature_moyenne": get_average_temperature(annee),
                    "nom_gagnant": nom_gagnant,
                    "prenom_gagnant": prenom_gagnant,
                    "nom_perdant": nom_perdant,
                    "prenom_perdant": prenom_perdant,
                    "pourcentage_vote_gagnant": np.float64(pourcentage_vote_gagnant),
                    # "pourcentage_vote_perdant": pourcentage_vote_perdant,  # Uncomment if needed
                    "pourcentage_vote_blanc": np.float64(pourcentage_vote_blanc), 
                    "pourcentage_abstention": np.float64(pourcentage_abstention),
                    "type_de_position": get_position_type(nom_gagnant, prenom_gagnant)
                })

    # Donn√©es num√©riques √† standardiser pour √©viter qu'une valeur soit sup√©rieure √† une autre lors de l'entrainement du mod√®le
    numerical_columns = ['moyenne_age', 'moyenne_pouvoir_achat', 'taux_chomage', 'temperature_moyenne', 
                        'pourcentage_vote_gagnant', 'pourcentage_vote_blanc', 'pourcentage_abstention']
    
    # Inutile avec Forest Classifier, mais recommand√© dans le cas o√π l'on voudrait utiliser diff√©rents algorithmes
    STANDARDIZED_DATA = standardize_data(DATA_TO_STANDARDIZE, numerical_columns)

    # Insertion en BDD
    insert_rows_to_db(STANDARDIZED_DATA)


if __name__ == "__main__":
    process_vote_files()
